"""
–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—â–∏–π —Å–µ—Ä–≤–∏—Å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ k
"""

from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from scipy.interpolate import interp1d
from scipy.fft import fft, fftfreq
from scipy.stats import variation, moment
from scipy.stats.mstats import gmean
import requests
import time
import os
import json
import joblib
from typing import Dict, List, Optional, Tuple, Union
import random
from scipy.stats import kurtosis, skew, entropy
import nolds  # –¥–ª—è —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
import antropy as ent  # –¥–ª—è —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã—Ö –º–µ—Ä
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification

# –§–∏–∫—Å–∏—Ä—É–µ–º –≤—Å–µ —Å–∏–¥—ã –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
random.seed(SEED)
load_dotenv()

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name()}")
    print(f"   –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

class EvaluationMetrics:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""

    def __init__(self, horizons: List[int] = [1, 20]):
        self.horizons = sorted(horizons)
        self.max_horizon = max(horizons)

    def calculate_targets(self, df: pd.DataFrame, price_col: str = 'close') -> Dict[int, pd.Series]:
        """–†–∞—Å—á–µ—Ç —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        targets = {}
        for k in self.horizons:
            targets[k] = (df[price_col].shift(-k) / df[price_col] - 1)
        return targets

    def calculate_mae(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞"""
        return np.mean(np.abs(y_true - y_pred))

    def calculate_brier(self, y_true: np.ndarray, prob_up: np.ndarray) -> float:
        """Brier score –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞"""
        binary_targets = (y_true > 0).astype(float)
        return np.mean((binary_targets - prob_up) ** 2)

    def calculate_direction_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """–î–æ–ª—è –≤–µ—Ä–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞–∫–∞ (Direction Accuracy)"""
        correct_signs = np.sign(y_true) == np.sign(y_pred)
        return correct_signs.mean()

    def normalize_metrics(self, mae: float, brier: float,
                         mae_base: float, brier_base: float) -> Tuple[float, float]:
        """–ù–æ—Ä–º–∏—Ä–æ–≤–∫–∞ –º–µ—Ç—Ä–∏–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–µ–π–∑–ª–∞–π–Ω–∞"""
        mae_norm = max(0, 1 - (mae / mae_base)) if mae_base > 0 else 0
        brier_norm = max(0, 1 - (brier / brier_base)) if brier_base > 0 else 0
        return mae_norm, brier_norm

    def calculate_final_score(self, y_true: np.ndarray, y_pred: np.ndarray,
                            prob_up: np.ndarray, mae_base: float, brier_base: float) -> Dict:
        """–†–∞—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å–∫–æ—Ä–∞ –ø–æ —Ñ–æ—Ä–º—É–ª–µ"""

        mae = self.calculate_mae(y_true, y_pred)
        brier = self.calculate_brier(y_true, prob_up)
        da = self.calculate_direction_accuracy(y_true, y_pred)

        mae_norm, brier_norm = self.normalize_metrics(mae, brier, mae_base, brier_base)

        # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
        da_component = 0.1 * (1 / da) if da > 0 else 0
        final_score = 0.7 * mae_norm + 0.3 * brier_norm + da_component

        return {
            'mae': mae,
            'brier': brier,
            'direction_accuracy': da,
            'mae_norm': mae_norm,
            'brier_norm': brier_norm,
            'final_score': final_score
        }

class BaselineModel:
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è Baseline –º–æ–¥–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ k
    """

    def __init__(self, horizons: List[int] = [1, 20], window_size: int = 5):
        self.horizons = sorted(horizons)
        self.window_size = window_size

    def compute_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–º–æ–º–µ–Ω—Ç—É–º, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)"""
        df_processed = df.copy()

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–∫–µ—Ä–∞–º
        for ticker in df_processed['ticker'].unique():
            mask = df_processed['ticker'] == ticker
            ticker_data = df_processed[mask].copy()

            # 1. –ú–æ–º–µ–Ω—Ç—É–º = –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ window_size –¥–Ω–µ–π
            ticker_data['momentum'] = (
                ticker_data['close'].pct_change(self.window_size)
            )

            # 2. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å = std –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π –∑–∞ window_size –¥–Ω–µ–π
            ticker_data['volatility'] = (
                ticker_data['close'].pct_change().rolling(self.window_size).std()
            )

            # 3. –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –∑–∞ window_size –¥–Ω–µ–π
            ticker_data['ma'] = ticker_data['close'].rolling(self.window_size).mean()

            # 4. –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç MA (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ)
            ticker_data['distance_from_ma'] = (
                (ticker_data['close'] - ticker_data['ma']) / ticker_data['ma']
            )

            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            df_processed.loc[mask, 'momentum'] = ticker_data['momentum'].values
            df_processed.loc[mask, 'volatility'] = ticker_data['volatility'].values
            df_processed.loc[mask, 'ma'] = ticker_data['ma'].values
            df_processed.loc[mask, 'distance_from_ma'] = ticker_data['distance_from_ma'].values

        return df_processed

    def predict(self, df: pd.DataFrame) -> Tuple[np.ndarray, Dict[int, np.ndarray]]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤

        Returns:
            predictions: –º–∞—Å—Å–∏–≤ [return_k1, return_k2, ..., logit_k1, logit_k2, ...]
            prob_up_dict: —Å–ª–æ–≤–∞—Ä—å {k: prob_up_k}
        """
        # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        df_with_features = self.compute_features(df)

        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –Ω—É–ª—è–º–∏ (–¥–ª—è –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≥–¥–µ –Ω–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏)
        df_with_features['momentum'] = df_with_features['momentum'].fillna(0)
        df_with_features['volatility'] = df_with_features['volatility'].fillna(0.01)
        df_with_features['distance_from_ma'] = df_with_features['distance_from_ma'].fillna(0)

        predictions_list = []
        prob_up_dict = {}

        for k in self.horizons:
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –º–æ–º–µ–Ω—Ç—É–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—Å—è
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
            scaling_factor = min(1.0, k / 5.0)  # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ —Å–∏–ª—å–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç

            returns_k = df_with_features['momentum'] * scaling_factor

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞
            def sigmoid(x, sensitivity=10):
                return 1 / (1 + np.exp(-sensitivity * x))

            # –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
            sensitivity = max(5, 15 - k)  # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ –º–µ–Ω—å—à–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            prob_up_k = sigmoid(df_with_features['momentum'], sensitivity=sensitivity)

            # Clipping: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0.1, 0.9] –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            prob_up_k = prob_up_k.clip(0.1, 0.9)

            # Clipping: –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑—É–º–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            max_return = min(0.5, k * 0.05)  # –ó–∞–≤–∏—Å–∏—Ç –æ—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
            returns_k = returns_k.clip(-max_return, max_return)

            predictions_list.extend([returns_k])
            prob_up_dict[k] = prob_up_k

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        all_returns = np.column_stack(predictions_list)

        # –î–æ–±–∞–≤–ª—è–µ–º logits –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª—å—é
        all_logits = []
        for k in self.horizons:
            prob_up_k = prob_up_dict[k]
            logit_k = np.log(prob_up_k / (1 - prob_up_k))
            all_logits.append(logit_k)

        all_logits = np.column_stack(all_logits)
        predictions = np.column_stack([all_returns, all_logits])

        return predictions, prob_up_dict

    @staticmethod
    def naive_forecast(df: pd.DataFrame, horizon: int = 20) -> Tuple[np.ndarray, np.ndarray]:
        """–ù–∞–∏–≤–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑: —Å—Ä–µ–¥–Ω—è—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å"""
        returns = df['close'].pct_change().dropna()
        avg_return = returns.mean() if len(returns) > 0 else 0

        y_pred = np.full(len(df), avg_return)
        prob_up = np.full(len(df), 0.5)

        return y_pred, prob_up


class FinBertNewsProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FinBERT –º–æ–¥–µ–ª–∏"""

    def __init__(self, model_name: str = "yiyanghkust/finbert-tone"):
        self.model_name = model_name
        self.device = device
        self.max_length = 512

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        print(f"üì∞ –ó–∞–≥—Ä—É–∑–∫–∞ FinBERT –º–æ–¥–µ–ª–∏: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()

        # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è FinBERT
        self.embedding_dim = 768  # BERT base model

        print(f"‚úÖ FinBERT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print(f"   –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.embedding_dim}")

    def get_news_embeddings(self, texts: List[str], batch_size: int = 16) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é FinBERT"""
        if len(texts) == 0:
            return np.zeros((0, self.embedding_dim))

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
        valid_texts = []
        valid_indices = []

        for i, text in enumerate(texts):
            if text and len(text.strip()) > 0:
                valid_texts.append(text.strip())
                valid_indices.append(i)

        if len(valid_texts) == 0:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            return np.zeros((len(texts), self.embedding_dim))

        embeddings = np.zeros((len(texts), self.embedding_dim))

        # –û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
        total_batches = (len(valid_texts) + batch_size - 1) // batch_size

        print(f"üì° –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ FinBERT...")
        print(f"   –ú–æ–¥–µ–ª—å: {self.model_name}, –¢–µ–∫—Å—Ç–æ–≤: {len(valid_texts)}/{len(texts)}, –ë–∞—Ç—á–µ–π: {total_batches}")

        for i in range(0, len(valid_texts), batch_size):
            batch_texts = valid_texts[i:i + batch_size]
            batch_indices = valid_indices[i:i + batch_size]
            batch_num = i // batch_size + 1

            try:
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors="pt"
                ).to(self.device)

                # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º [CLS] —Ç–æ–∫–µ–Ω –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
                    batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏
                    for j, idx in enumerate(batch_indices):
                        embeddings[idx] = batch_embeddings[j]

                print(f"   ‚úÖ –ë–∞—Ç—á {batch_num}/{total_batches} –æ–±—Ä–∞–±–æ—Ç–∞–Ω ({len(batch_texts)} —Ç–µ–∫—Å—Ç–æ–≤)")

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {batch_num}: {e}")
                # –î–ª—è –Ω–µ—É–¥–∞—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –æ—Å—Ç–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏

        print(f"‚úÖ –í—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã!")
        return embeddings


    def get_sentiment_scores(self, texts: List[str], batch_size: int = 16) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ sentiment scores —Å –ø–æ–º–æ—â—å—é FinBERT (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"""
        if len(texts) == 0:
            return np.zeros((0, 3))  # positive, negative, neutral

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        sentiment_model = AutoModelForSequenceClassification.from_pretrained(self.model_name).to(self.device)
        sentiment_model.eval()

        sentiment_scores = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]

            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt"
            ).to(self.device)

            with torch.no_grad():
                outputs = sentiment_model(**inputs)
                probs = F.softmax(outputs.logits, dim=-1).cpu().numpy()
                sentiment_scores.extend(probs)

        return np.array(sentiment_scores)


class HybridTransformerModel(nn.Module):
    """–ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å: Transformer + FinBERT + –ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""

    def __init__(self, ts_input_size: int, llm_embedding_dim: int, horizons: List[int] = [1, 20], hidden_size: int = 256):
        super().__init__()
        self.horizons = sorted(horizons)
        self.num_horizons = len(horizons)

        # –ö–∞–∂–¥—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç 2 –≤—ã—Ö–æ–¥–∞: return –∏ logit
        output_size = 2 * self.num_horizons

        self.ts_transformer = TimeSeriesTransformer(
            input_size=ts_input_size,
            d_model=128,
            nhead=8,
            num_layers=3
        )

        self.llm_projection = nn.Linear(llm_embedding_dim, 128)

        total_input_size = 128 + 128

        self.fusion_network = nn.Sequential(
            nn.Linear(total_input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size // 2, output_size)
        )

    def forward(self, timeseries, news_embeddings):
        ts_features = self.ts_transformer(timeseries)
        news_features = self.llm_projection(news_embeddings)
        combined = torch.cat([ts_features, news_features], dim=1)
        output = self.fusion_network(combined)
        return output


class TimeSeriesDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""

    def __init__(self, features, news_embeddings, targets_dict: Dict[int, np.ndarray], seq_length=30):
        self.features = features
        self.news_embeddings = news_embeddings
        self.targets_dict = targets_dict
        self.horizons = sorted(targets_dict.keys())
        self.seq_length = seq_length

    def __len__(self):
        return len(self.features) - self.seq_length

    def __getitem__(self, idx):
        features_seq = self.features[idx:idx+self.seq_length]
        news_embedding = self.news_embeddings[idx+self.seq_length]

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤
        targets = []
        for horizon in self.horizons:
            target = self.targets_dict[horizon][idx+self.seq_length]
            targets.append(target)

        return (
            torch.FloatTensor(features_seq),
            torch.FloatTensor(news_embedding),
            torch.FloatTensor(targets)
        )


class FinancialForecastService:
    """–°–µ—Ä–≤–∏—Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ k"""

    def __init__(self, model_dir: str = "model_artifacts", seq_length: int = 30,
                 k_days: List[int] = [1, 20]):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        self.seq_length = seq_length
        self.k_days = sorted(k_days)
        self.device = device

        self.scaler = StandardScaler()
        self.news_processor = FinBertNewsProcessor()  # ‚úÖ –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ FinBERT
        self.feature_generator = FeatureGenerator()
        self.evaluator = EvaluationMetrics(horizons=self.k_days)
        self.baseline_model = BaselineModel(horizons=self.k_days, window_size=5)
        self.model = None

        self.feature_columns = None
        self.selected_features = None
        # –ë–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
        self.mae_base_dict = {k: 0.01 for k in self.k_days}
        self.brier_base_dict = {k: 0.25 for k in self.k_days}

        print(f"üéØ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å–µ—Ä–≤–∏—Å —Å –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º–∏: {self.k_days}")

    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞"""
        print("üîß –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        all_features = []

        for ticker in df['ticker'].unique():
            ticker_data = df[df['ticker'] == ticker].copy().sort_values('begin')

            if len(ticker_data) < 30:
                continue

            prices = ticker_data['close'].values
            dates = pd.DatetimeIndex(ticker_data['begin'])

            ticker_features = self.feature_generator.create_features(prices, dates)

            ticker_features['ticker'] = ticker
            ticker_features['begin'] = dates

            all_features.append(ticker_features)

        if not all_features:
            return df

        features_df = pd.concat(all_features, ignore_index=True)

        result_df = pd.merge(df, features_df, on=['ticker', 'begin'], how='left')

        result_df = result_df.fillna(method='ffill').fillna(method='bfill')

        print(f"   ‚úì –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(features_df.columns) - 2} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        return result_df

    def prepare_news_features(self, news_df: pd.DataFrame, candles_df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FinBERT"""
        print("üì∞ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ FinBERT...")

        if news_df is None or len(news_df) == 0:
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –Ω—É–ª–µ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
            zero_embeddings = [np.zeros(768) for _ in range(len(candles_df))]
            candles_df = candles_df.copy()
            candles_df['news_embedding'] = zero_embeddings
            return candles_df

        news_df = news_df.copy()
        news_df['publish_date'] = pd.to_datetime(news_df['publish_date'])
        news_df['date'] = news_df['publish_date'].dt.date

        news_df['full_text'] = news_df['title'] + ". " + news_df['publication']

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–∞—Ç–µ
        grouped_news = news_df.groupby(['date'])['full_text'].apply(
            lambda x: ' '.join(x) if len(x) > 0 else ""
        ).reset_index()

        news_texts = grouped_news['full_text'].tolist()
        print(f"   üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ FinBERT –¥–ª—è {len(news_texts)} –≥—Ä—É–ø–ø –Ω–æ–≤–æ—Å—Ç–µ–π...")
        news_embeddings = self.news_processor.get_news_embeddings(news_texts)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ DataFrame
        grouped_news['news_embedding'] = list(news_embeddings)

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º candles_df –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        candles_df = candles_df.copy()
        candles_df['date'] = pd.to_datetime(candles_df['begin']).dt.date

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        merged_df = pd.merge(
            candles_df,
            grouped_news[['date', 'news_embedding']],
            on='date',
            how='left'
        )

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω—É–ª—è–º–∏ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ß–ê–°–¢–¨
        mask = merged_df['news_embedding'].isna()
        if mask.any():
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –Ω—É–ª–µ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
            zero_embeddings_list = [np.zeros(768) for _ in range(mask.sum())]

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ Series —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º
            zero_embeddings_series = pd.Series(zero_embeddings_list, index=merged_df[mask].index)

            # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
            merged_df.loc[mask, 'news_embedding'] = zero_embeddings_series

        print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(merged_df)} —Å—Ç—Ä–æ–∫ —Å –Ω–æ–≤–æ—Å—Ç–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏")
        return merged_df

    def prepare_targets(self, df: pd.DataFrame) -> Dict[int, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ –ø–æ —Ñ–æ—Ä–º—É–ª–µ: R_{t+N} = close_{t+N} / close_t - 1"""
        df = df.sort_values(['ticker', 'begin']).copy()
        targets_dict = {}

        for k in self.k_days:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–∫–µ—Ä—É –∏ –≤—ã—á–∏—Å–ª—è–µ–º –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∑–∞ k –¥–Ω–µ–π
            targets = df.groupby('ticker').apply(
                lambda x: (x['close'].shift(-k) / x['close'] - 1)
            ).reset_index(level=0, drop=True)

            targets_dict[k] = targets

        return targets_dict

    def _calculate_baseline_metrics(self, df: pd.DataFrame) -> Tuple[Dict, Dict]:
        """–†–∞—Å—á–µ—Ç –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        print("üìä –†–∞—Å—á–µ—Ç –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≤—Å–µ—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤...")

        eval_df = df.tail(max(100, len(df) // 5)).copy()

        if len(eval_df) < 10:
            print("   ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –±–µ–π–∑–ª–∞–π–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            return {k: 0.01 for k in self.k_days}, {k: 0.25 for k in self.k_days}

        mae_base_dict = {}
        brier_base_dict = {}

        for k in self.k_days:
            actual_return_col = f'actual_return_{k}d'
            eval_df[actual_return_col] = (eval_df['close'].shift(-k) / eval_df['close'] - 1)
            eval_df_clean = eval_df.dropna(subset=[actual_return_col])

            if len(eval_df_clean) == 0:
                print(f"   ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞ {k}, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
                mae_base_dict[k] = 0.01
                brier_base_dict[k] = 0.25
                continue

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π baseline –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫
            baseline_predictions, baseline_probs_dict = self.baseline_model.predict(eval_df_clean)
            y_true = eval_df_clean[actual_return_col].values

            # –ò–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
            horizon_idx = self.k_days.index(k)
            y_pred_baseline = baseline_predictions[:, horizon_idx]
            prob_up_baseline = baseline_probs_dict[k]

            mae_base = self.evaluator.calculate_mae(y_true, y_pred_baseline)
            brier_base = self.evaluator.calculate_brier(y_true, prob_up_baseline)

            mae_base_dict[k] = mae_base
            brier_base_dict[k] = brier_base

            print(f"   ‚úÖ –ì–æ—Ä–∏–∑–æ–Ω—Ç {k} –¥–Ω–µ–π: MAE={mae_base:.6f}, Brier={brier_base:.6f}")

        return mae_base_dict, brier_base_dict

    def select_features(self, X_train, y_train, X_val, y_val):
        """–û—Ç–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("üéØ –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        self.selected_features, scores = FeatureSelector.forward_selection(
            X_train, y_train, X_val, y_val, max_features=50, min_improvement=0.001
        )

        return self.selected_features

    def train(self, candles_df: pd.DataFrame, news_df: pd.DataFrame, val_ratio: float = 0.2):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
        print(f"üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤: {self.k_days}...")

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏ —á—Ç–æ–±—ã –Ω–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        candles_df = candles_df.copy()
        if news_df is not None:
            news_df = news_df.copy()

        candles_df = self.create_features(candles_df)
        targets_dict = self.prepare_targets(candles_df)

        full_df = self.prepare_news_features(news_df, candles_df)

        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —Ç–∞—Ä–≥–µ—Ç–∞—Ö
        target_columns = [f'target_{k}d' for k in self.k_days]
        for k in self.k_days:
            full_df[f'target_{k}d'] = targets_dict[k]

        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –≤—Å–µ —Ç–∞—Ä–≥–µ—Ç—ã NaN
        full_df = full_df.dropna(subset=target_columns)

        if len(full_df) == 0:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ NaN")
            return

        # –†–∞—Å—á–µ—Ç –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫
        self.mae_base_dict, self.brier_base_dict = self._calculate_baseline_metrics(full_df)

        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        numeric_columns = full_df.select_dtypes(include=[np.number]).columns.tolist()
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        exclude_columns = target_columns + ['news_embedding']
        self.feature_columns = [col for col in numeric_columns if col not in exclude_columns]

        print(f"   –î–æ—Å—Ç—É–ø–Ω–æ {len(self.feature_columns)} —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ train/val
        full_df = full_df.sort_values('begin')
        split_idx = int(len(full_df) * (1 - val_ratio))
        train_df = full_df.iloc[:split_idx]
        val_df = full_df.iloc[split_idx:]

        print(f"   Train samples: {len(train_df)}, Val samples: {len(val_df)}")

        if len(train_df) == 0 or len(val_df) == 0:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è/–≤–∞–ª–∏–¥–∞—Ü–∏–∏")
            return

        # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞...
        X_train = train_df[self.feature_columns].values
        X_val = val_df[self.feature_columns].values

        self.scaler.fit(X_train)
        X_train_scaled = self.scaler.transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        y_train = train_df[f'target_{self.k_days[0]}d'].values
        y_val = val_df[f'target_{self.k_days[0]}d'].values

        self.selected_features = self.select_features(X_train_scaled, y_train, X_val_scaled, y_val)

        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(self.selected_features)} –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ numpy array
        news_embeddings = np.stack(full_df['news_embedding'].values)

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        train_targets_dict = {}
        val_targets_dict = {}
        for k in self.k_days:
            train_targets_dict[k] = train_df[f'target_{k}d'].values
            val_targets_dict[k] = val_df[f'target_{k}d'].values

        self.model = HybridTransformerModel(
            ts_input_size=len(self.selected_features),
            llm_embedding_dim=news_embeddings.shape[1],  # FinBERT embedding size
            horizons=self.k_days,
            hidden_size=256
        ).to(self.device)

        self._train_model(train_df, val_df, train_targets_dict, val_targets_dict, news_embeddings)

        self._save_artifacts()

        print("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")


    def _train_model(self, train_df: pd.DataFrame, val_df: pd.DataFrame,
                    train_targets_dict: Dict, val_targets_dict: Dict, news_embeddings: np.ndarray):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º–∏"""
        print("üß† –û–±—É—á–µ–Ω–∏–µ Transformer –º–æ–¥–µ–ª–∏...")

        X_train = self.scaler.transform(train_df[self.feature_columns].values)
        X_train_selected = X_train[:, self.selected_features]

        train_news_embeddings = news_embeddings[:len(train_df)]

        train_dataset = TimeSeriesDataset(
            X_train_selected, train_news_embeddings,
            train_targets_dict, self.seq_length
        )
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        criterion = nn.MSELoss()

        self.model.train()
        for epoch in range(20):
            total_loss = 0
            for batch_ts, batch_news, batch_targets in train_loader:
                batch_ts = batch_ts.to(self.device)
                batch_news = batch_news.to(self.device)
                batch_targets = batch_targets.to(self.device)

                optimizer.zero_grad()

                outputs = self.model(batch_ts, batch_news)

                # –†–∞–∑–¥–µ–ª—è–µ–º –≤—ã—Ö–æ–¥—ã –Ω–∞ returns –∏ logits
                num_horizons = len(self.k_days)
                pred_returns = outputs[:, :num_horizons]
                # pred_logits = outputs[:, num_horizons:]  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∫–∞

                loss = criterion(pred_returns, batch_targets)

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                total_loss += loss.item()

            if epoch % 5 == 0:
                val_loss = self._validate(val_df, val_targets_dict, news_embeddings[len(train_df):])
                print(f"   Epoch {epoch}, Train Loss: {total_loss/len(train_loader):.6f}, Val Loss: {val_loss:.6f}")

    def _validate(self, val_df: pd.DataFrame, val_targets_dict: Dict, val_news_embeddings: np.ndarray) -> float:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.model.eval()

        X_val = self.scaler.transform(val_df[self.feature_columns].values)
        X_val_selected = X_val[:, self.selected_features]

        total_loss = 0
        criterion = nn.MSELoss()
        num_horizons = len(self.k_days)

        with torch.no_grad():
            for i in range(len(X_val_selected) - self.seq_length):
                ts_seq = torch.FloatTensor(X_val_selected[i:i+self.seq_length]).unsqueeze(0).to(self.device)
                news_embed = torch.FloatTensor(val_news_embeddings[i+self.seq_length]).unsqueeze(0).to(self.device)

                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã
                targets = []
                for k in self.k_days:
                    target_val = val_targets_dict[k][i+self.seq_length]
                    targets.append(target_val)
                batch_targets = torch.FloatTensor([targets]).to(self.device)

                output = self.model(ts_seq, news_embed)
                pred_returns = output[:, :num_horizons]

                loss = criterion(pred_returns, batch_targets)
                total_loss += loss.item()

        self.model.train()
        return total_loss / (len(X_val_selected) - self.seq_length)

    def predict(self, candles_df: pd.DataFrame, news_df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ sample_submission.csv"""
        if self.model is None:
            self._load_artifacts()

        print(f"üéØ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ 1-20 –¥–Ω–µ–π...")

        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ (–¥–∞—Ç–∞ t)
        latest_data = candles_df.sort_values(['ticker', 'begin']).groupby('ticker').last().reset_index()
        print(f"   –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –¥–∞—Ç—É: {latest_data['begin'].iloc[0]}")

        # –°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–∞—Ç—ã
        candles_with_features = self.create_features(candles_df)
        full_df = self.prepare_news_features(news_df, candles_with_features)

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–∞—Ç—ã –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞
        predictions_data = []

        for ticker in latest_data['ticker'].unique():
            ticker_data = full_df[full_df['ticker'] == ticker].sort_values('begin')

            if len(ticker_data) < self.seq_length:
                print(f"   ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–∏–∫–µ—Ä–∞ {ticker}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã seq_length
            X_ticker = self.scaler.transform(ticker_data[self.feature_columns].values)
            X_selected = X_ticker[-self.seq_length:, self.selected_features]

            news_embedding = ticker_data['news_embedding'].values[-1]

            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
            self.model.eval()
            with torch.no_grad():
                ts_seq = torch.FloatTensor(X_selected).unsqueeze(0).to(self.device)
                news_embed = torch.FloatTensor(news_embedding).unsqueeze(0).to(self.device)

                output = self.model(ts_seq, news_embed)
                prediction = output.cpu().squeeze().numpy()

            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π (–ø–µ—Ä–≤—ã–µ num_horizons –∑–Ω–∞—á–µ–Ω–∏–π)
            num_horizons = len(self.k_days)
            returns_prediction = prediction[:num_horizons]

            predictions_data.append({
            'ticker': ticker,
                'predictions': returns_prediction
            })

        if not predictions_data:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
            return pd.DataFrame()

        # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ sample_submission.csv
        output_rows = []

        for item in predictions_data:
            ticker = item['ticker']
            predictions = item['predictions']

            row = {'ticker': ticker}

            # –°–æ–∑–¥–∞–µ–º 20 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (p1-p20)
            # –ï—Å–ª–∏ —É –Ω–∞—Å –º–µ–Ω—å—à–µ 20 –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤, –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ–º
            if len(predictions) >= 20:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 20 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                for i in range(20):
                    row[f'p{i+1}'] = float(predictions[i])
            else:
                # –ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ–º –∏–º–µ—é—â–∏–µ—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–æ 20
                available_horizons = self.k_days[:len(predictions)]
                available_predictions = predictions

                # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ç–æ—Ä

                try:
                    # –õ–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º
                    interp_func = interp1d(
                        available_horizons,
                        available_predictions,
                        kind='linear',
                        fill_value='extrapolate'
                    )

                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ 1-20
                    for i in range(1, 21):
                        row[f'p{i}'] = float(interp_func(i))

                except:
                    # –ï—Å–ª–∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –¥—É–±–ª–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    for i in range(1, 21):
                        if i <= len(predictions):
                            row[f'p{i}'] = float(predictions[i-1])
                        else:
                            row[f'p{i}'] = float(predictions[-1])

            output_rows.append(row)

        result_df = pd.DataFrame(output_rows)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∫–∞–∫ –≤ sample_submission.csv
        if not result_df.empty:
            all_tickers = ['AFLT', 'ALRS', 'CHMF', 'GAZP', 'GMKN', 'LKOH', 'MAGN', 'MGNT',
                        'MOEX', 'MTSS', 'NVTK', 'PHOR', 'PLZL', 'ROSN', 'RUAL', 'SBER',
                        'SIBN', 'T', 'VTBR']

            # –°–æ–∑–¥–∞–µ–º DataFrame —Å–æ –≤—Å–µ–º–∏ —Ç–∏–∫–µ—Ä–∞–º–∏
            full_result = pd.DataFrame({'ticker': all_tickers})
            full_result = full_result.merge(result_df, on='ticker', how='left')

            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Ç–∏–∫–µ—Ä—ã –Ω—É–ª–µ–≤—ã–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
            for i in range(1, 21):
                col = f'p{i}'
                if col not in full_result.columns:
                    full_result[col] = 0.0
                else:
                    full_result[col] = full_result[col].fillna(0.0)

            print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è {len(result_df)} —Ç–∏–∫–µ—Ä–æ–≤")
            return full_result
        else:
            return pd.DataFrame()

    def evaluate_predictions(self, candles_df: pd.DataFrame, predictions: pd.DataFrame) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ p1-p20"""
        print("üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")

        evaluation_results = {}

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ –¥–∞—Ç–µ
        candles_sorted = candles_df.sort_values(['ticker', 'begin']).copy()

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞ –æ—Ç 1 –¥–æ 20 –¥–Ω–µ–π
        for k in range(1, 21):
            return_col = f'p{k}'

            if return_col not in predictions.columns:
                print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ {return_col}")
                continue

            # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∑–∞ k –¥–Ω–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞
            actual_returns = []
            predicted_returns = []

            for ticker in predictions['ticker'].unique():
                ticker_candles = candles_sorted[candles_sorted['ticker'] == ticker]
                ticker_pred = predictions[predictions['ticker'] == ticker]

                if len(ticker_candles) < 2 or len(ticker_pred) == 0:
                    continue

                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É (t) –∏ –¥–∞—Ç—É t+k
                last_date = ticker_candles['begin'].iloc[-1]
                close_t = ticker_candles['close'].iloc[-1]

                # –ò—â–µ–º —Ü–µ–Ω—É –Ω–∞ k –¥–Ω–µ–π –≤–ø–µ—Ä–µ–¥
                future_idx = -1 - k
                if abs(future_idx) <= len(ticker_candles):
                    close_t_k = ticker_candles['close'].iloc[future_idx]
                    actual_return = (close_t_k / close_t) - 1

                    actual_returns.append(actual_return)
                    predicted_returns.append(ticker_pred[return_col].iloc[0])

            if len(actual_returns) < 5:
                print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞ {k} –¥–Ω–µ–π")
                continue

            y_true = np.array(actual_returns)
            y_pred = np.array(predicted_returns)

            # –î–ª—è Brier score –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞
            prob_up = 1 / (1 + np.exp(-y_pred * 10))

            mae_base = self.mae_base_dict.get(k, 0.01)
            brier_base = self.brier_base_dict.get(k, 0.25)

            scores = self.evaluator.calculate_final_score(y_true, y_pred, prob_up, mae_base, brier_base)

            evaluation_results[k] = {
                'horizon': k,
                **scores,
                'num_samples': len(actual_returns)
            }

            print(f"   üìà –ì–æ—Ä–∏–∑–æ–Ω—Ç {k} –¥–Ω–µ–π:")
            print(f"      Final Score: {scores['final_score']:.4f}")
            print(f"      MAE: {scores['mae']:.6f}")
            print(f"      Samples: {len(actual_returns)}")

        return evaluation_results

    def _save_artifacts(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞—è –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫–∏ –∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã"""
        artifacts = {
            'model_state_dict': self.model.state_dict(),
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'selected_features': self.selected_features,
            'seq_length': self.seq_length,
            'k_days': self.k_days,  # ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã
            'mae_base_dict': self.mae_base_dict,
            'brier_base_dict': self.brier_base_dict
        }

        torch.save(artifacts, self.model_dir / 'model_artifacts.pth')
        print("üíæ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã (–≤–∫–ª—é—á–∞—è –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –∏ –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫–∏)")

    def _load_artifacts(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞—è –±–µ–π–∑–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫–∏ –∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã"""
        artifacts_path = self.model_dir / 'model_artifacts.pth'
        if not artifacts_path.exists():
            raise FileNotFoundError(f"–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {artifacts_path}")

        artifacts = torch.load(artifacts_path, map_location=self.device, weights_only=False)

        self.scaler = artifacts['scaler']
        self.feature_columns = artifacts['feature_columns']
        self.selected_features = artifacts['selected_features']
        self.seq_length = artifacts['seq_length']
        self.k_days = artifacts.get('k_days', [1, 20])  # ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã
        self.mae_base_dict = artifacts.get('mae_base_dict', {k: 0.01 for k in self.k_days})
        self.brier_base_dict = artifacts.get('brier_base_dict', {k: 0.25 for k in self.k_days})

        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        self.evaluator = EvaluationMetrics(horizons=self.k_days)
        self.baseline_model = BaselineModel(horizons=self.k_days, window_size=5)

        news_embeddings_dummy = np.zeros((1, 768))  # FinBERT embedding size
        self.model = HybridTransformerModel(
            ts_input_size=len(self.selected_features),
            llm_embedding_dim=news_embeddings_dummy.shape[1],
            horizons=self.k_days,  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã
            hidden_size=256
        ).to(self.device)

        self.model.load_state_dict(artifacts['model_state_dict'])
        print(f"üíæ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (–≥–æ—Ä–∏–∑–æ–Ω—Ç—ã: {self.k_days})")

# –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã (FeatureGenerator, FeatureSelector, PositionalEncoding, TimeSeriesTransformer) –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

class FeatureGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""

    @staticmethod
    def create_features(prices: pd.Series, dates: pd.DatetimeIndex) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        prices_series = pd.Series(prices, index=dates)

        features = pd.DataFrame(index=dates)

        # –õ–∞–≥–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        for lag in range(1, 7):
            features[f'lag_{lag}'] = prices_series.shift(lag)
            features[f'returns_{lag}'] = prices_series.pct_change(lag)
            features[f'log_{lag}'] = np.log(prices_series.shift(lag))
            features[f'diff_{lag}'] = prices_series - prices_series.shift(lag)

        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        for window in [3, 5, 7, 14, 21, 30]:
            features[f'ma_{window}'] = prices_series.rolling(window).mean()
            features[f'std_{window}'] = prices_series.rolling(window).std()
            features[f'median_{window}'] = prices_series.rolling(window).median()
            features[f'min_{window}'] = prices_series.rolling(window).min()
            features[f'max_{window}'] = prices_series.rolling(window).max()
            features[f'range_{window}'] = prices_series.rolling(window).max() - prices_series.rolling(window).min()
            features[f'var_{window}'] = prices_series.rolling(window).var()

        # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        for alpha in [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]:
            features[f'ema_alpha_{alpha}'] = prices_series.ewm(alpha=alpha, adjust=False).mean()
            ema1 = prices_series.ewm(alpha=alpha, adjust=False).mean()
            ema2 = ema1.ewm(alpha=alpha, adjust=False).mean()
            features[f'dema_alpha_{alpha}'] = 2 * ema1 - ema2

        # RSI (Relative Strength Index)
        for period in [14, 21, 30]:
            delta = prices_series.diff()
            gain = delta.where(delta > 0, 0).rolling(period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
            rs = gain / loss
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä
        for k_period in [14, 21, 30]:
            low_min = prices_series.rolling(k_period).min()
            high_max = prices_series.rolling(k_period).max()
            features[f'stoch_k_{k_period}'] = 100 * ((prices_series - low_min) / (high_max - low_min))
            features[f'stoch_d_{k_period}'] = features[f'stoch_k_{k_period}'].rolling(3).mean()

        # Williams %R
        for period in [14, 21, 30]:
            lowest_low = prices_series.rolling(period).min()
            highest_high = prices_series.rolling(period).max()
            features[f'williams_r_{period}'] = -100 * ((highest_high - prices_series) / (highest_high - lowest_low))

        # Rate of Change (ROC)
        for period in [5, 10, 14, 21]:
            features[f'roc_{period}'] = ((prices_series - prices_series.shift(period)) / prices_series.shift(
                period)) * 100

        # Momentum
        for period in [5, 10, 14, 21]:
            features[f'momentum_{period}'] = prices_series - prices_series.shift(period)

        # MACD
        ema12 = prices_series.ewm(span=12, adjust=False).mean()
        ema26 = prices_series.ewm(span=26, adjust=False).mean()
        features['macd'] = ema12 - ema26
        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()
        features['macd_histogram'] = features['macd'] - features['macd_signal']

        # –ü–æ–ª–æ—Å—ã –ë–æ–ª–ª–∏–Ω–¥–∂–µ—Ä–∞
        for period in [20]:
            ma = prices_series.rolling(period).mean()
            std = prices_series.rolling(period).std()
            features[f'bb_upper_{period}'] = ma + (std * 2)
            features[f'bb_middle_{period}'] = ma
            features[f'bb_lower_{period}'] = ma - (std * 2)
            features[f'bb_width_{period}'] = (features[f'bb_upper_{period}'] - features[f'bb_lower_{period}']) / ma
            features[f'bb_position_{period}'] = (prices_series - features[f'bb_lower_{period}']) / (
                        features[f'bb_upper_{period}'] - features[f'bb_lower_{period}'])

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['hour'] = dates.hour
        features['dayofweek'] = dates.dayofweek
        features['dayofmonth'] = dates.day
        features['month'] = dates.month
        features['quarter'] = dates.quarter

        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏
        features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)
        features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)
        features['dayofweek_sin'] = np.sin(2 * np.pi * features['dayofweek'] / 7)
        features['dayofweek_cos'] = np.cos(2 * np.pi * features['dayofweek'] / 7)
        features['month_sin'] = np.sin(2 * np.pi * features['month'] / 12)
        features['month_cos'] = np.cos(2 * np.pi * features['month'] / 12)

        # –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['is_weekend'] = features['dayofweek'].isin([5, 6]).astype(int)
        features['is_month_start'] = (features['dayofmonth'] == 1).astype(int)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–º–µ–Ω—Ç—ã
        for window in [10, 20, 50]:
            features[f'skew_{window}'] = prices_series.rolling(window).apply(
                lambda x: skew(x) if len(x) > 2 else np.nan)
            features[f'kurtosis_{window}'] = prices_series.rolling(window).apply(
                lambda x: kurtosis(x) if len(x) > 3 else np.nan)
            features[f'cv_{window}'] = prices_series.rolling(window).apply(
                lambda x: variation(x) if len(x) > 1 and np.std(x) > 0 else np.nan)

        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π)
        for window in [64]:
            features[f'spectral_mean_{window}'] = prices_series.rolling(window).apply(
                lambda x: np.mean(np.abs(fft(x))) if len(x) == window else np.nan
            )

        return features


class FeatureSelector:
    """–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é Forward Selection"""

    @staticmethod
    def forward_selection(X_train, y_train, X_val, y_val, max_features=50, min_improvement=0.001):
        """Forward Selection –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""

        n_features = X_train.shape[1]
        selected_features = []
        remaining_features = list(range(n_features))
        scores = []
        prev_score = -np.inf

        print(f"üéØ –ù–∞—á–∞–ª–æ Forward Selection (–º–∞–∫—Å–∏–º—É–º {max_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")

        for step in range(min(max_features, n_features)):
            best_score = -np.inf
            best_feature = None

            # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏
            for feature in remaining_features:
                # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—É—â–∏–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                current_features = selected_features + [feature]

                # –û–±—É—á–∞–µ–º –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é
                model = LinearRegression()
                model.fit(X_train[:, current_features], y_train)

                # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                score = model.score(X_val[:, current_features], y_val)

                if score > best_score:
                    best_score = score
                    best_feature = feature

            if prev_score != -np.inf:
                improvement = best_score - prev_score
            else:
                improvement = best_score

            # –î–æ–±–∞–≤–ª—è–µ–º –ª—É—á—à–∏–π –ø—Ä–∏–∑–Ω–∞–∫
            selected_features.append(best_feature)
            remaining_features.remove(best_feature)
            scores.append(best_score)

            print(f"   –®–∞–≥ {step + 1}: –¥–æ–±–∞–≤–ª–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫ {best_feature}, R¬≤ = {best_score:.4f}")

            # –£—Å–ª–æ–≤–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if improvement < min_improvement and step > 10:  # –ú–∏–Ω–∏–º—É–º 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                print(f"   ‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞: —É–ª—É—á—à–µ–Ω–∏–µ –º–µ–Ω–µ–µ {min_improvement * 100:.1f}%")
                break

            prev_score = best_score

        print(f"‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        return selected_features, scores


class PositionalEncoding(nn.Module):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Transformer"""

    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]


class TimeSeriesTransformer(nn.Module):
    """Transformer –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""

    def __init__(self, input_size, d_model=64, nhead=8, num_layers=3, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.input_projection = nn.Linear(input_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.input_projection(x) * np.sqrt(self.d_model)
        x = self.pos_encoding(x)
        x = self.transformer(x)
        x = self.dropout(x)
        return x[:, -1, :]


def load_data(candles_paths: List[str], news_paths: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤"""
    candles_dfs = []
    for path in candles_paths:
        if Path(path).exists():
            df = pd.read_csv(path)
            df['begin'] = pd.to_datetime(df['begin'])
            candles_dfs.append(df)
            print(f"   üìä –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∫–æ—Ç–∏—Ä–æ–≤–∫–∏: {path} ({len(df)} —Å—Ç—Ä–æ–∫)")
        else:
            print(f"   ‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

    candles_df = pd.concat(candles_dfs, ignore_index=True) if candles_dfs else pd.DataFrame()

    news_dfs = []
    for path in news_paths:
        if Path(path).exists():
            df = pd.read_csv(path)
            df['publish_date'] = pd.to_datetime(df['publish_date'])
            news_dfs.append(df)
            print(f"   üì∞ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç–∏: {path} ({len(df)} —Å—Ç—Ä–æ–∫)")
        else:
            print(f"   ‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

    news_df = pd.concat(news_dfs, ignore_index=True) if news_dfs else pd.DataFrame()

    return candles_df, news_df


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤"""
    import argparse

    parser = argparse.ArgumentParser(description='–°–µ—Ä–≤–∏—Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è')
    parser.add_argument('--mode', type=str, required=True, choices=['train', 'predict', 'evaluate'],
                       help='–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: train, predict –∏–ª–∏ evaluate')
    parser.add_argument('--candles', type=str, nargs='+', required=True,
                       help='–ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –∫–æ—Ç–∏—Ä–æ–≤–∫–∞–º–∏ (candles.csv, candles_2.csv, etc)')
    parser.add_argument('--news', type=str, nargs='+', required=True,
                       help='–ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ (news.csv, news_2.csv, etc)')
    parser.add_argument('--k_days', type=int, nargs='+', default=[1, 20],
                       help='–ì–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–Ω—è—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä: 1 5 20)')
    parser.add_argument('--output', type=str, default='predictions.csv',
                       help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏')
    parser.add_argument('--model_dir', type=str, default='model_artifacts',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –º–æ–¥–µ–ª–∏')
    parser.add_argument('--evaluate', action='store_true',
                       help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –æ—Ü–µ–Ω–∫—É –ø–æ—Å–ª–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')

    args = parser.parse_args()

    print("=" * 70)
    print("üöÄ –°–ï–†–í–ò–° –§–ò–ù–ê–ù–°–û–í–û–ì–û –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –° –ü–†–û–ò–ó–í–û–õ–¨–ù–´–ú–ò –ì–û–†–ò–ó–û–ù–¢–ê–ú–ò")
    print("=" * 70)
    print(f"üéØ –ó–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã: {args.k_days}")

    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    candles_df, news_df = load_data(args.candles, args.news)

    if candles_df.empty:
        print("‚ùå –ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫")
        return

    print(f"   ‚úì –í—Å–µ–≥–æ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫: {len(candles_df)}")
    print(f"   ‚úì –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news_df) if not news_df.empty else 0}")
    print(f"   ‚úì –¢–∏–∫–µ—Ä—ã: {candles_df['ticker'].unique().tolist()}")

    # ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º–∏
    service = FinancialForecastService(model_dir=args.model_dir, k_days=args.k_days)

    if args.mode == 'train':
        print(f"\nüéØ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –î–õ–Ø –ì–û–†–ò–ó–û–ù–¢–û–í {args.k_days}...")
        service.train(candles_df, news_df)

    elif args.mode == 'predict':
        print(f"\nüéØ –ó–ê–ü–£–°–ö –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø –î–õ–Ø –ì–û–†–ò–ó–û–ù–¢–û–í {args.k_days}...")
        predictions = service.predict(candles_df, news_df)

        if not predictions.empty:
            predictions.to_csv(args.output, index=False)
            print(f"üíæ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {args.output}")
            print(f"üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {list(predictions.columns)}")

            if args.evaluate:
                print("\nüìä –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò...")
                evaluation_results = service.evaluate_predictions(candles_df, predictions)

                if evaluation_results:
                    eval_df = pd.DataFrame([evaluation_results])
                    eval_df.to_csv('evaluation_results.csv', index=False)
                    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: evaluation_results.csv")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")

    elif args.mode == 'evaluate':
        print("\nüìä –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò...")
        if Path(args.output).exists():
            predictions = pd.read_csv(args.output)
            evaluation_results = service.evaluate_predictions(candles_df, predictions)

            if evaluation_results:
                eval_df = pd.DataFrame([evaluation_results])
                eval_df.to_csv('evaluation_results.csv', index=False)
                print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: evaluation_results.csv")
        else:
            print("‚ùå –§–∞–π–ª —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")

    print("\n" + "=" * 70)
    print("‚úÖ –í–´–ü–û–õ–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 70)

if __name__ == "__main__":
    main()